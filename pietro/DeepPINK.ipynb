{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.mlab as mlab\n",
    "from scipy.linalg import qr\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, merge, LocallyConnected1D, Flatten, Conv1D,LeakyReLU\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "#from keras.objectives import mse\n",
    "from keras import regularizers, optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import Constant\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler,LabelEncoder\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV,KFold\n",
    "from sklearn.metrics import accuracy_score,cohen_kappa_score,roc_auc_score,average_precision_score\n",
    "\n",
    "\n",
    "batch_size = 64;\n",
    "filterNum = 1;\n",
    "bias = True;\n",
    "iterNum = 1;\n",
    "\n",
    "\n",
    "def show_layer_info(layer_name, layer_out):\n",
    "    # print('[layer]: %s\\t[shape]: %s \\n' % (layer_name,str(layer_out.get_shape().as_list())))\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def build_DNN(p, coeff=0):\n",
    "\n",
    "    input = Input(name='input', shape=(p, 2));\n",
    "    show_layer_info('Input', input);\n",
    "\n",
    "    local1 = LocallyConnected1D(filterNum,1, use_bias=bias, kernel_initializer=Constant(value=0.1))(input);\n",
    "    show_layer_info('LocallyConnected1D', local1);\n",
    "\n",
    "    local2 = LocallyConnected1D(1,1, use_bias=bias, kernel_initializer='glorot_normal')(local1);\n",
    "    show_layer_info('LocallyConnected1D', local2);\n",
    "\n",
    "    flat = Flatten()(local2);\n",
    "    show_layer_info('Flatten', flat);\n",
    "\n",
    "    dense1 = Dense(int(h_size),use_bias=bias, kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l1(coeff))(flat)\n",
    "    x = LeakyReLU(0.2)(dense1)\n",
    "\n",
    "    dense2 = Dense(int(h_size),use_bias=bias, kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l1(coeff))(x)\n",
    "    x = LeakyReLU(0.2)(dense2)\n",
    "\n",
    "    dense3 = Dense(int(h_size),use_bias=bias, kernel_initializer='glorot_normal', kernel_regularizer=regularizers.l1(coeff))(x)\n",
    "    x = LeakyReLU(0.2)(dense3)\n",
    "    \n",
    "    out_ = Dense(1, activation='sigmoid', kernel_initializer='glorot_normal')(x)\n",
    "\n",
    "\n",
    "    model = Model(inputs=input, outputs=out_)\n",
    "    \n",
    "    \n",
    "    opt = optimizers.Adam(learning_rate=0.0005, epsilon=1e-6)\n",
    "    \n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_DNN(model, X, y, myCallback):\n",
    "    num_sequences = len(y);\n",
    "    num_positives = np.sum(y);\n",
    "    num_negatives = num_sequences - num_positives;\n",
    "    \n",
    "     \n",
    "    model.fit(X, y, epochs=num_epochs, batch_size=batch_size, verbose=1, validation_split = 0.2,class_weight={True: num_sequences / num_positives, False: num_sequences / num_negatives}, callbacks=[myCallback]);\n",
    "    return model;\n",
    "\n",
    "\n",
    "def test_DNN(model, X, y):\n",
    "    return roc_auc_score(y, model.predict(X));\n",
    "\n",
    "def predict_DNN(model, X):\n",
    "    return model.predict(X);\n",
    "\n",
    "def plot_roc(model, X, y):\n",
    "    fpr, tpr, thresholds = roc_curve(y_true=y, y_score=model.predict(X));\n",
    "    roc_auc = auc(x=fpr, y=tpr)\n",
    "    plt.plot(fpr, tpr, linestyle='-', label='auc={:.4f}'.format(roc_auc));\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', linewidth=2)\n",
    "    plt.legend(loc='lower right', fontsize=16)\n",
    "    plt.xlim([-0.1, 1.1])\n",
    "    plt.ylim([-0.1, 1.1])\n",
    "    plt.ylabel('True Positive Rate', fontsize=20)\n",
    "    plt.xlabel('False Positive Rate', fontsize=20)\n",
    "    plt.xticks(fontsize=18)\n",
    "    plt.yticks(fontsize=18)\n",
    "    plt.show();\n",
    "\n",
    "class My_Callback(keras.callbacks.Callback):\n",
    "    def __init__(self, outputDir, pVal):\n",
    "        self.outputDir = outputDir;\n",
    "        self.pVal = pVal;\n",
    "        print(self.outputDir);\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if ((epoch+1) % 100) != 0: return;\n",
    "\n",
    "        h_local1_weight = np.array(self.model.layers[1].get_weights()[0]);\n",
    "        h_local2_weight = np.array(self.model.layers[2].get_weights()[0]);\n",
    "\n",
    "        print('h_local1_weight = ' + str(h_local1_weight.shape))\n",
    "        print('h_local2_weight = ' + str(h_local2_weight.shape))\n",
    "        h0 = np.zeros((self.pVal, 2));\n",
    "        h0_abs = np.zeros((self.pVal, 2));\n",
    "\n",
    "        for pIdx in range(self.pVal):\n",
    "            h0[pIdx, :] = np.matmul(h_local1_weight[pIdx, :, :], h_local2_weight[pIdx, :, :]).flatten();\n",
    "            h0_abs[pIdx, :] = np.matmul(np.fabs(h_local1_weight[pIdx, :, :]), np.fabs(h_local2_weight[pIdx, :, :])).flatten();\n",
    "\n",
    "        print('h0 = ' + str(h0.shape))\n",
    "        print('h0_abs = ' + str(h0_abs.shape))\n",
    "\n",
    "        h1 = np.array(self.model.layers[4].get_weights()[0]);\n",
    "        h2 = np.array(self.model.layers[6].get_weights()[0]);\n",
    "        h3 = np.array(self.model.layers[8].get_weights()[0]); \n",
    "        h4 = np.array(self.model.layers[10].get_weights()[0]) \n",
    "\n",
    "        print('h1 = ' + str(h1.shape))\n",
    "        print('h2 = ' + str(h2.shape))\n",
    "        print('h3 = ' + str(h3.shape))\n",
    "        print('h4 = ' + str(h3.shape)) \n",
    "\n",
    "        W1 = h1;\n",
    "        W_curr = h1;\n",
    "        W2 = np.matmul(W_curr, h2);\n",
    "        W_curr = np.matmul(W_curr, h2); \n",
    "        W3 = np.matmul(W_curr, h3); \n",
    "        W_curr = np.matmul(W_curr, h3)\n",
    "        W4 = np.matmul(W_curr, h4) \n",
    "  \n",
    "        print('W1 = ' + str(W1.shape))\n",
    "        print('W2 = ' + str(W2.shape))\n",
    "        print('W3 = ' + str(W3.shape))\n",
    "        v0_h0 = h0[:, 0].reshape((self.pVal, 1));\n",
    "        v1_h0 = h0[:, 1].reshape((self.pVal, 1));\n",
    "        v0_h0_abs = h0_abs[:, 0].reshape((self.pVal, 1));\n",
    "        v1_h0_abs = h0_abs[:, 1].reshape((self.pVal, 1));\n",
    "\n",
    "        \n",
    "        v3 = np.vstack((np.sum(np.square(np.multiply(v0_h0_abs, np.fabs(W3))), axis=1).reshape((self.pVal, 1)), np.sum(np.square(np.multiply(v1_h0_abs, np.fabs(W3))), axis=1).reshape((self.pVal, 1)))).T;\n",
    "\n",
    "        v5 = np.vstack((np.sum(np.multiply(v0_h0, W3), axis=1).reshape((self.pVal, 1)),\n",
    "                        np.sum(np.multiply(v1_h0, W3), axis=1).reshape((self.pVal, 1)))).T;\n",
    "        \n",
    "        df = pd.DataFrame(data=v3.flatten())\n",
    "        df.to_csv(self.outputDir) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cv_deepPINK(xOr,xK,yvalues,n_split,model_name):\n",
    "    \n",
    "    \n",
    "    xvalues = np.concatenate((xOr,xK),axis=1)\n",
    "    kf = KFold(n_splits=n_split)\n",
    "\n",
    "    \n",
    "    y_preds = []\n",
    "    aucc = []\n",
    "    auprc = []\n",
    "\n",
    "    pVal = int(xvalues.shape[1] / 2)\n",
    "    n = xvalues.shape[0]\n",
    "\n",
    "    X_origin = xvalues[:, :pVal]\n",
    "    X_knockoff = xvalues[:, pVal:]\n",
    "\n",
    "    \n",
    "    fold = 0\n",
    "    for tr_idx, te_idx in kf.split(X_origin):\n",
    "        \n",
    "        X_tr_o, X_te_o = X_origin[tr_idx], X_origin[te_idx]\n",
    "        X_tr_k, X_te_k = X_knockoff[tr_idx], X_knockoff[te_idx]\n",
    "        y_tr, y_te = yvalues[tr_idx], yvalues[te_idx]\n",
    "    \n",
    "        n_tr = X_tr_o.shape[0]\n",
    "        n_te = X_te_o.shape[0]\n",
    "        \n",
    "        x3D_train = np.zeros((n_tr, pVal, 2))\n",
    "        x3D_train[:, :, 0] = X_tr_o\n",
    "        x3D_train[:, :, 1] = X_tr_k\n",
    "        x3D_test = np.zeros((n_te, pVal, 2))\n",
    "        x3D_test[:, :, 0] = X_te_o\n",
    "        x3D_test[:, :, 1] = X_te_k\n",
    "        label_train = y_tr\n",
    "        label_test = y_te\n",
    "    \n",
    "        \n",
    "        coeff = 0.05 * np.sqrt(2.0 * np.log(pVal) / n)\n",
    "        dataDir = '/Users/utente/Documents/università/tesi - confronto FS/analisi 2-512 features'\n",
    "        outputDir = os.path.join( dataDir, 'CV_' + model_name + '_fold' + str(fold))\n",
    "       \n",
    "        \n",
    "        DNN = build_DNN(pVal, coeff)\n",
    "        model = DNN\n",
    "\n",
    "        myCallback = My_Callback(outputDir,pVal)\n",
    "        DNN = train_DNN(DNN, x3D_train, label_train, myCallback)\n",
    "        \n",
    "        hist_df = pd.DataFrame(model.history.history)\n",
    "        \n",
    "        y_pred=predict_DNN(model, x3D_test)\n",
    "        \n",
    "        fold += 1\n",
    "        \n",
    "        aucc.append(roc_auc_score(label_test,y_pred))\n",
    "        auprc.append(average_precision_score(label_test, y_pred))\n",
    "        \n",
    "        \n",
    "    return np.mean(aucc), np.var(aucc),np.mean(auprc),np.var(auprc)\n",
    "\n",
    "def set_data(path,data_name,k_feat):\n",
    "    \n",
    "    data = np.genfromtxt(path+ data_name + str(k_feat) + 'feat.csv',delimiter=',')\n",
    "\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "def matches(best_feat,non_zero):\n",
    "    #non_zero = non_zeros.copy()\n",
    "    match = []\n",
    "    for i in range(len(best_feat)):\n",
    "        for j in range(len(non_zero)):\n",
    "            if best_feat[i] == non_zero[j]:\n",
    "                match.append(best_feat[i])\n",
    "    umatch = []\n",
    "    sorted_match = np.sort(match)\n",
    "    for i in range(len(sorted_match)):\n",
    "        if i == len(sorted_match)-1:\n",
    "        \n",
    "            umatch.append(sorted_match[i])\n",
    "        \n",
    "        if  i < len(sorted_match)-1 and sorted_match[i] < sorted_match[i+1] :\n",
    "        \n",
    "            umatch.append(sorted_match[i])\n",
    "        \n",
    "    return umatch#,match\n",
    "\n",
    "def get_features(model_names,n_split,k):                     \n",
    "    \n",
    "    best_feat = np.zeros((n_split,k))\n",
    "                     \n",
    "    \n",
    "        \n",
    "    for i in range(n_split):\n",
    "    \n",
    "        file = './DeepPINK/CV_' + model_names[1] + '_fold' + str(i) \n",
    "        imp = np.genfromtxt(file,delimiter=',')\n",
    "        imp = imp[1:,1]\n",
    "            \n",
    "        p = int(imp.shape[0]/2)\n",
    "           \n",
    "        importances = imp[:p]-imp[p:]\n",
    "                     \n",
    "                     \n",
    "        best_feat[i] = importances.argsort()[-k:][::-1]\n",
    "        \n",
    "    return best_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '/Users/utente/Documents/università/tesi - confronto FS/analisi 2-512 features'\n",
    "\n",
    "path='./data/'\n",
    "data_name='ring-xor-sum_1000samples-'\n",
    "DK_or_2o = '_DK'\n",
    "h_size = 64\n",
    "num_epochs = 1000\n",
    "splits = 6\n",
    "comments = '_3layers64'\n",
    "dataset = 'ring-xor-sum'\n",
    "tot_feats = [6,8,16,32,64,128,256,512]\n",
    "\n",
    "cv_auc = []\n",
    "var_auc = []\n",
    "cv_auprc = []\n",
    "var_auprc = []\n",
    "\n",
    "for i in tot_feats:\n",
    "    \n",
    "    # import original data and knockoff features\n",
    "    X_original,y = set_data(path,data_name,i)\n",
    "    X_2o = np.loadtxt(dataDir+'/data/2o_knockoff.csv',delimiter=',')\n",
    "    X_DK = np.loadtxt(dataDir+'/data/DK_knockoff.csv',delimiter=',')\n",
    "    \n",
    "    # multiply input by 100 (I obtained better results in this way)\n",
    "    X_original = X_original*100\n",
    "    X_2o = X_2o[:1000,:i]*100\n",
    "    X_DK = X_DK[:1000,:i]*100\n",
    "\n",
    "    if DK_or_2o == '_DK':\n",
    "        X_knock = X_DK\n",
    "    else:\n",
    "        X_knock = X_2o\n",
    "    \n",
    "    auccc,vauccc,auprc,vauprc = cv_deepPINK(X_original,X_knock,y,splits,dataset+'_'+str(i)+DK_or_2o+comments)\n",
    "\n",
    "    var_auc.append(sauccc)\n",
    "    cv_auc.append(auccc)\n",
    "    var_auprc.append(sauprc)\n",
    "    cv_auprc.append(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = 6\n",
    "K_feat = 6\n",
    "name0 = 'DeepPINK'\n",
    "dataset = 'ring-xor-sum_'\n",
    "\n",
    "feat_res = []\n",
    "feat_res_2k = []\n",
    "\n",
    "\n",
    "# Best K features\n",
    "\n",
    "for i in tot_feats:\n",
    "    feat_res.append(get_features([name0,dataset+str(i)+DK_or_2o+comments],splits,K_feat))\n",
    "    \n",
    "m = np.zeros((len(tot_feats),splits)) #rows=ring2,ring4,ring8... columns=fold0,fold1,fold2...\n",
    "for j in range(len(tot_feats)):\n",
    "    for k in range(splits):\n",
    "        m[j,k] =len(matches(feat_res[j][k],np.arange(K_feat)))\n",
    "\n",
    "# best K feat averaged on the 6 folds\n",
    "cv_feat = np.mean(m,axis=1) \n",
    "var_feat = np.var(m,axis=1)\n",
    "\n",
    "# Best 2K features\n",
    "\n",
    "for i in tot_feats:\n",
    "    if i == 6 or i == 8:\n",
    "        feat_res_2k.append(get_features([name0,dataset+str(i)+name1],splits,int(K_feat)))\n",
    "    else:\n",
    "        feat_res_2k.append(get_features([name0,dataset+str(i)+name1],splits,int(2*K_feat)))\n",
    "\n",
    "m = np.zeros((len(tot_feats),splits)) #rows=ring2,ring4,ring8... columns=fold0,fold1,fold2...\n",
    "for j in range(len(tot_feats)):\n",
    "    for k in range(splits):\n",
    "        \n",
    "        m[j,k] =len(matches(feat_res_2k[j][k],np.arange(int(K_feat))))\n",
    "        \n",
    "# best 2K feat averaged on the 6 folds\n",
    "cv_feat_2k = np.mean(m,axis=1)\n",
    "var_feat_2k = np.var(m,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "df = pd.DataFrame([cv_auc,var_auc,cv_auprc,var_auprc,cv_feat,var_feat,cv_feat_2k,var_feat_2k])\n",
    "df1 = df.T\n",
    "df1\n",
    "df1.to_excel(\"Auc-Auprc-KFeat-2KFeat_DeepPINK\"+DK_or_2o+\"_\"+dataset[:-1]+\".xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
