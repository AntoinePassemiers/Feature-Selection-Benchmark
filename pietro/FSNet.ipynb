{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oBgKgGYMyKp4"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Layer, Dense, Dropout, Input, LeakyReLU\n",
    "from keras.layers.core import Activation\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.initializers import Constant, glorot_normal\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "import random\n",
    "import sys\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.metrics import cohen_kappa_score,roc_auc_score,accuracy_score,average_precision_score\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rEX90HGh4X61"
   },
   "outputs": [],
   "source": [
    "# set parameters\n",
    "\n",
    "bins=10\n",
    "batch_size=64\n",
    "start_temp=10.0\n",
    "min_temp=0.1\n",
    "lossWeights = {\"recon\":1, \"classacc\":100}\n",
    "losses = {\"recon\": \"mean_squared_error\", \"classacc\": \"binary_crossentropy\",}\n",
    "cmi=0\n",
    "mi=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AWGt82Axwcui"
   },
   "outputs": [],
   "source": [
    "## Definition of FSNet method\n",
    "\n",
    "def calc_MI(X,Y,bins):\n",
    "    c_XY = np.histogram2d(X,Y,bins)[0]\n",
    "    c_X = np.histogram(X,bins)[0]\n",
    "    c_Y = np.histogram(Y,bins)[0]\n",
    "    H_X = shan_entropy(c_X)\n",
    "    H_Y = shan_entropy(c_Y)\n",
    "    H_XY = shan_entropy(c_XY)\n",
    "    mi1 = H_X + H_Y - H_XY\n",
    "    return mi1\n",
    "\n",
    "def shan_entropy(c):\n",
    "    c_normalized = c / float(np.sum(c))\n",
    "    c_normalized = c_normalized[np.nonzero(c_normalized)]\n",
    "    H = -sum(c_normalized* np.log2(c_normalized))  \n",
    "    return H\n",
    "\n",
    "def MI(S):\n",
    "    bins = 10\n",
    "    n = S.shape[1]\n",
    "    mis=0\n",
    "    count=0\n",
    "    for ix in np.arange(n):\n",
    "        for jx in np.arange(ix+1,n):\n",
    "            mis = mis+calc_MI(S[:,ix], S[:,jx], bins)\n",
    "            count=count+1\n",
    "    mis=mis/count\n",
    "    return mis\n",
    "\n",
    "\n",
    "class tinyLayerE(Layer):\n",
    "    def __init__(self, output_dim, u, bins, start_temp=10.0, min_temp=0.1, alpha=0.99999, **kwargs):\n",
    "        self.output_dim=output_dim\n",
    "        self.u=K.constant(u)\n",
    "        self.start_temp = start_temp\n",
    "        self.min_temp = K.constant(min_temp)\n",
    "        self.alpha = K.constant(alpha)\n",
    "        super(tinyLayerE, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self,input_shape):\n",
    "        self.temp = self.add_weight(name = 'temp', shape = [], initializer = Constant(self.start_temp), trainable = False)\n",
    "        \n",
    "        self.tinyW=self.add_weight(name='tinyW', shape=(bins,self.output_dim), initializer='uniform', trainable=True)\n",
    "        super(tinyLayerE,self).build(input_shape)\n",
    "\n",
    "    def call(self, X, training = None):\n",
    "        \n",
    "        al=K.softmax(K.dot(self.u,self.tinyW))\n",
    "        al=K.transpose(al) \n",
    "        logits=K.log(10*K.maximum(K.minimum(al,0.9999999),K.epsilon()))\n",
    "        uniform = K.random_uniform(logits.shape, K.epsilon(), 1.0)\n",
    "        gumbel = -K.log(-K.log(uniform))\n",
    "        temp = K.update(self.temp, K.maximum(self.min_temp, self.temp * self.alpha))\n",
    "        noisy_logits = (logits+gumbel) / temp\n",
    "        samples = K.softmax(noisy_logits)\n",
    "        discrete_logits = K.one_hot(K.argmax(logits), logits.shape[1])\n",
    "        self.logits=samples\n",
    "        dl = np.zeros(self.logits.shape)\n",
    "        p = K.get_value(self.logits)\n",
    "        \n",
    "        for i in range(dl.shape[0]):\n",
    "            ind = np.argmax(p, axis=None)\n",
    "            x=ind//dl.shape[1]\n",
    "            y=ind%dl.shape[1]\n",
    "            dl[x][y]=1\n",
    "            p[x]=-np.ones(dl.shape[1])\n",
    "            p[:,y]=-np.ones(dl.shape[0])\n",
    "            discrete_logits = K.one_hot(K.argmax(K.variable(dl)), dl.shape[1])\n",
    "        \n",
    "        self.selections = K.in_train_phase(samples, discrete_logits, training)\n",
    "        Y = K.dot(X, K.transpose(self.selections))\n",
    "        \n",
    "        return Y\n",
    "\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n",
    "\n",
    "class tinyLayerD(Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, u, bins, **kwargs):\n",
    "        self.output_dim=output_dim\n",
    "        self.u=K.constant(u)\n",
    "        super(tinyLayerD, self).__init__(**kwargs)\n",
    "  \n",
    "    def build(self,input_shape):\n",
    "        self.tinyW=self.add_weight(name='tinyW', shape=(bins, input_shape[1]), initializer='uniform', trainable=True)\n",
    "        super(tinyLayerD,self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        weights=K.transpose(K.tanh(K.dot(self.u,self.tinyW)))\n",
    "        return K.dot(x,weights)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.output_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LT2J8KLzwcup"
   },
   "outputs": [],
   "source": [
    "def set_data(path,data_name,k_feat):\n",
    "    \n",
    "    data = np.genfromtxt(path+ data_name + str(k_feat) + 'feat.csv',delimiter=',')\n",
    "\n",
    "    X = data[:,:-1]\n",
    "    y = data[:,-1]\n",
    "    \n",
    "    return X,y\n",
    "\n",
    "def matches(best_feat,non_zero):\n",
    "    match = []\n",
    "    for i in range(len(best_feat)):\n",
    "        for j in range(len(non_zero)):\n",
    "            if best_feat[i] == non_zero[j]:\n",
    "                match.append(best_feat[i])\n",
    "    return match\n",
    "\n",
    "def get_features(model_names,n_split,k):                     \n",
    "    \n",
    "    best_feat = np.zeros((n_split,k))\n",
    "       \n",
    "    for i in range(n_split):\n",
    "            \n",
    "        imp = spio.loadmat('/Users/utente/Documents/università/tesi - confronto FS/analisi 2-512 features/FSNet/'+model_names[1] + \"_fold_\" + str(i) + \"_indices.mat\")\n",
    "        best_feat[i] = imp['indices']\n",
    "        \n",
    "    return best_feat\n",
    "\n",
    "\n",
    "def cv_fsnet(X,Y,num_exp,k_feat,l,h_size,num_epochs,dataDir,model_name = 'fs'):\n",
    "    \n",
    "    # num_exp: number of folds for KFold cross val\n",
    "    # l: learning rate\n",
    "    # k_feat: Number of features to select\n",
    "    # h_size: neurons per layer\n",
    "    \n",
    "    opt=RMSprop(learning_rate=l)    \n",
    "    \n",
    "    kf = KFold(n_splits=num_exp)\n",
    "    \n",
    "    # Normalization to N(0,1)\n",
    "    \n",
    "    \n",
    "    X=np.delete(X,np.where(np.std(X,axis=0)==0),axis=1)\n",
    "    for i in range(X.shape[1]):\n",
    "        if np.max(X[:,i])!=0:\n",
    "            X[:,i]=X[:,i]/np.max(np.absolute(X[:,i]))\n",
    "            mu_Xi=np.mean(X[:,i])\n",
    "            std_Xi=np.std(X[:,i])\n",
    "            X[:,i]=X[:,i]-mu_Xi\n",
    "            if std_Xi!=0:\n",
    "                X[:,i]=X[:,i]/std_Xi\n",
    "  \n",
    "    aucc = []\n",
    "    auprc = []\n",
    "    fold = 0\n",
    "    for tr_idx, te_idx in kf.split(X):\n",
    "        \n",
    "        x_train, x_test = X[tr_idx], X[te_idx]\n",
    "        y_train, y_test = Y[tr_idx], Y[te_idx]\n",
    "        \n",
    "        u_train=np.zeros([x_train.shape[1],bins],dtype=float)\n",
    "        for i in range(0,x_train.shape[1]):\n",
    "            hist=np.histogram(x_train[:,i],bins)\n",
    "            for j in range(0,bins):\n",
    "                u_train[i,j]=hist[0][j]*0.5*(hist[1][j]+hist[1][j+1])\n",
    "\n",
    "        steps_per_epoch = (len(x_train) + batch_size - 1) // batch_size\n",
    "        alpha = math.exp(math.log(min_temp / start_temp) / (num_epochs * steps_per_epoch))\n",
    "       \n",
    "  ################################\n",
    "  # FsNet\n",
    "  ################################\n",
    "\n",
    "        inp1=Input(shape=(x_train.shape[1],))\n",
    "        x=tinyLayerE(k_feat,u_train,bins,start_temp, min_temp, alpha, name = 'tinyLayerE')(inp1)\n",
    "        \n",
    "        x = Dense(int(h_size))(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dense(int(h_size))(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dense(int(h_size))(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        \n",
    "        \n",
    "        x1 = Dense(int(h_size))(x)\n",
    "        x1 = LeakyReLU(0.2)(x1)\n",
    "        x1 = Dense(int(h_size))(x1)\n",
    "        x1 = LeakyReLU(0.2)(x1)\n",
    "        x1 = Dense(int(h_size))(x1)\n",
    "        x1 = LeakyReLU(0.2)(x1)\n",
    "        \n",
    "        \n",
    "        x1 = tinyLayerD(x_train.shape[1],u_train,bins,name = 'recon')(x1)\n",
    "        x2 = Dense(1,activation=\"sigmoid\", name=\"classacc\")(x)\n",
    "        model = Model(inputs=inp1, outputs=[x1, x2])\n",
    "        callback_early = EarlyStopping(monitor='val_classacc_loss', patience=100,restore_best_weights=True)\n",
    "        #reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.33,patience=3, min_lr=0.00001)\n",
    "        model.compile(optimizer=opt, loss=losses, loss_weights=lossWeights, metrics=[\"accuracy\",\"mse\"])\n",
    "        \n",
    "        model.summary()\n",
    "       \n",
    "        history = model.fit(x_train, {\"recon\": x_train, \"classacc\": y_train},validation_split=0.2 ,epochs=num_epochs, callbacks=[callback_early])#validation_data=(x_test, {\"recon\": x_test, \"classacc\": y_test})\n",
    "        \n",
    "\n",
    "        x_pred,y_pred = model.predict(x_test)\n",
    "        \n",
    "        aucc.append(roc_auc_score(y_test,y_pred))\n",
    "        auprc.append(average_precision_score(y_test, y_pred))\n",
    "       \n",
    "        outputDir = os.path.join( dataDir, 'FSNet')\n",
    "        try:\n",
    "            os.stat(outputDir)\n",
    "        except:\n",
    "            os.mkdir(outputDir)\n",
    "\n",
    "        \n",
    "        with open(os.path.join(outputDir, model_name + '_fold_'+ str(i) +'_ypredproba.csv'), \"a+\") as myfile:\n",
    "            myfile.write(','.join([str(x) for x in y_pred.flatten()]) + '\\n')\n",
    "            \n",
    "        probabilities = K.get_value(K.softmax(model.get_layer('tinyLayerE').logits))\n",
    "        dl=np.zeros(model.get_layer('tinyLayerE').logits.shape)\n",
    "        p=K.get_value(model.get_layer('tinyLayerE').logits)\n",
    "        for j in range(dl.shape[0]):\n",
    "            ind=np.argmax(p,axis=None)\n",
    "            x=ind//dl.shape[1]\n",
    "            y=ind%dl.shape[1]\n",
    "            dl[x][y]=1\n",
    "            p[x]=-np.ones(dl.shape[1])\n",
    "            p[:,y]=-np.ones(dl.shape[0])\n",
    "\n",
    "        indices = K.get_value(K.argmax(dl))\n",
    "\n",
    "        spio.savemat(dataDir+'/FSNet/'+model_name + \"_fold_\" + str(fold) + '_indices.mat', {'indices': indices})\n",
    "        fold += 1\n",
    "\n",
    "    with open(os.path.join(outputDir, 'cv_cancelout_' + model_name + '_auc.csv'), \"a+\") as myfile:\n",
    "        myfile.write(str(aucc))\n",
    "    with open(os.path.join(outputDir, 'cv_cancelout_' + model_name + '_auc.csv'), \"a+\") as myfile:\n",
    "        myfile.write(str(auprc))\n",
    "        \n",
    "    return np.mean(aucc), np.mean(auprc) ,  np.var(aucc), np.var(auprc)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "97mMtCOEwcuw",
    "outputId": "e3066e05-7f5d-4eec-97e9-f32ad31f7010"
   },
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "cacc=np.zeros(num_epochs)\n",
    "acc=np.zeros(num_epochs)\n",
    "closs=np.zeros(num_epochs)\n",
    "loss=np.zeros(num_epochs)\n",
    "\n",
    "dataDir = '/Users/utente/Documents/università/tesi - confronto FS/analisi 2-64 features'\n",
    "\n",
    "path='./data/'\n",
    "data_name='ring-xor-sum_1000samples-' #RING: 'ring_1000samples-'; XOR: 'xor_1000samples-'; RING+XOR: 'ring+xor_1000samples-'; RING+XOR+SUM: 'ring-xor-sum_1000samples-'\n",
    "h_size = 64\n",
    "splits = 6\n",
    "lr = 0.005    \n",
    "K_feat = 6\n",
    "tot_feats = [6,8,16,32,64,128,256,512]\n",
    "dataset = 'ring-xor-sum_K_'\n",
    "\n",
    "cv_auc = []\n",
    "var_auc = []\n",
    "cv_auprc = []\n",
    "var_auprc = []\n",
    "\n",
    "for i in tot_feats:\n",
    "    \n",
    "    X,y = set_data(path,data_name,i)\n",
    "\n",
    "    auccc,auprc,vauccc,vauprc = cv_fsnet(X,y,splits,K_feat,lr,h_size,num_epochs,dataDir,dataset+str(i)+'_fs')\n",
    "\n",
    "    var_auc.append(vauccc)\n",
    "    cv_auc.append(auccc)\n",
    "    var_auprc.append(vauprc)\n",
    "    cv_auprc.append(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Auw5IIaUwcux"
   },
   "outputs": [],
   "source": [
    "name0 = 'Fsnet'\n",
    "name1 = '_fs'\n",
    "tot_feats = [6,8,16,32,64,128,256,512]\n",
    "\n",
    "feat_res = []\n",
    "feat_res_2k = []\n",
    "\n",
    "# Best K features\n",
    "\n",
    "for i in tot_feats:\n",
    "    feat_res.append(get_features([name0,dataset+str(i)+name1],splits,K_feat))\n",
    "    \n",
    "m = np.zeros((len(tot_feats),splits)) #rows=ring2,ring4,ring8... columns=fold0,fold1,fold2...\n",
    "for j in range(len(tot_feats)):\n",
    "    for k in range(splits):\n",
    "        m[j,k] =len(matches(feat_res[j][k],np.arange(K_feat)))\n",
    "\n",
    "# best K feat averaged on the 6 folds\n",
    "cv_feat = np.mean(m,axis=1) \n",
    "var_feat = np.var(m,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_8sI1BOuwcu0"
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "df = pd.DataFrame([cv_auc,var_auc,cv_auprc,var_auprc,cv_feat,var_feat])\n",
    "df1 = df.T\n",
    "df1\n",
    "df1.to_excel(\"Acc-Cohen-Auc-Auprc_FSNET_K_RING-XOR-SUM.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SL2Wq2Ngwcu1",
    "outputId": "6e3907de-d2d5-4e6c-926d-2a65c8da0325"
   },
   "outputs": [],
   "source": [
    "# Run the model selecting 2k features\n",
    "\n",
    "K_feat = 12\n",
    "tot_feats = [16,32,64,128,256,512]\n",
    "dataset = 'ring-xor-sum_2K_'\n",
    "\n",
    "cv_auc = []\n",
    "var_auc = []\n",
    "cv_auprc = []\n",
    "var_auprc = []\n",
    "\n",
    "for i in tot_feats:\n",
    "    \n",
    "    X,y = set_data(path,data_name,i)\n",
    "\n",
    "    auccc,auprc,vauccc,vauprc = cv_fsnet(X,y,splits,K_feat,lr,h_size,num_epochs,dataDir,dataset+str(i)+'_fs')\n",
    "\n",
    "    var_auc.append(vauccc)\n",
    "    cv_auc.append(auccc)\n",
    "    var_auprc.append(vauprc)\n",
    "    cv_auprc.append(auprc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zv5m-Avcwcu4",
    "outputId": "9768722b-9f6b-47d6-bfc6-9c8ee42d70b6"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Best 2K features\n",
    "\n",
    "for i in tot_feats:\n",
    "        feat_res_2k.append(get_features([name0,dataset+str(i)+name1],splits,int(K_feat)))\n",
    "\n",
    "m = np.zeros((len(tot_feats),splits)) #rows=ring2,ring4,ring8... columns=fold0,fold1,fold2...\n",
    "for j in range(len(tot_feats)):\n",
    "    for k in range(splits):\n",
    "        \n",
    "        m[j,k] =len(matches(feat_res_2k[j][k],np.arange(int(K_feat/2))))\n",
    "        \n",
    "# best 2K feat averaged on the 6 folds\n",
    "cv_feat_2k = np.mean(m,axis=1)\n",
    "var_feat_2k = np.var(m,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SZRi9H1ewcu4"
   },
   "outputs": [],
   "source": [
    "# Save the results\n",
    "\n",
    "df = pd.DataFrame([cv_auc,var_auc,cv_auprc,var_auprc,cv_feat_2k,var_feat_2k])\n",
    "df1 = df.T\n",
    "df1\n",
    "df1.to_excel(\"Acc-Cohen-Auc-Auprc_FSNET_2K_RING-XOR-SUM.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FSNet_final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
